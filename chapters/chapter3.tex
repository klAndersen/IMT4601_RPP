\chapter{Related work}
\label{chapter3:related_work}

%% 3-10 pages

\section{Comparison of \glsentrylong{ir} (\glsentryshort{ir}) when using Chat Agents and Search engines}
\label{chapter3:chatbot_vs_search_engine}
%% user experience with chatbots vs. search engines
I could not find anything in relation to this topic on the following sites:
\begin{itemize}
	\item \url{http://dblp.uni-trier.de/}
	\item \url{http://link.springer.com/}
	\item \url{http://ieeexplore.ieee.org/}
	\item \url{http://www.sciencedirect.com/}
	\item \url{http://dl.acm.org/}
\end{itemize}
The following is a list of the keyword searches that were made:
\begin{itemize}
	\item 'chatbot vs search engine'
	\item 'chatbot and search engine'
	\item 'comparison of chatbot and search engine'
	\item 'evaluation of retrieval systems' 
\end{itemize}
The reason for this might be related to the fact that search engines are able to retrieve all sorts of information from numerous documents and web-sites, whereas 
ChatBots are usually made for light conversation or for specific topics and purposes. The research question this is related to (Research question \ref{res_q1}) 
is more on the qualitative side, ie. will the users continue to use the search engine, or would they switch to the Chat Agent?
\vspace{0.5em}\newline
Although it is not an evaluation, in \citet{Crutzen2011} a comparison of the ChatBot Bzz (for Windows Live Messenger) is made against search engines and information lines. 
The goal is to see which is better at answering adolescents' questions related to sex, drugs, and alcohol. The comparison was done by giving the users a questionnaire with 
a 5-point Likert scale. The results showed that the users found the ChatBot to be faster and more anonymous, in addition to being easier to use. Information quantity was 
considered less then both information lines and search engines, and it performed better when it came to conciseness and information quality \citet[p.~517-518]{Crutzen2011}.
\vspace{0.5em}\newline
If one compares this paper to the goal of the Chat Agent I plan to develop, one can see some similarities. Even though search engines can give you numerous results, the quality 
of the results may vary, and there may not be any correlation between what you are looking for and what you find. Whereas with my Chat Agent, the focus is only on the StackExchange 
community, specifically on programming and StackOverflow. Therefore, one could also argue that rather then comparing the Chat Agent against a search engine, perhaps it rather should 
be compared against StackExchange. E.g.~comparing the results based on the question asked in the Chat Agent vs.~the question searched for on the given StackExchange site.
\vspace{0.5em}\newline
There is also a program called FAQ FINDER, which is documented in \citet{Burke1997}. As with my Chat Agent, here users can phrase their questions as they would when asking another 
person, rather then use keywords (as they perhaps would had to when using a search engine\footnote{It should also be noted that I am biased towards it being better to phrase questions 
	to find answers, rather then having to enter a list of keywords.}). 

\section{Chat Agents for Learning and Education}
\label{chapter3:learning_with_chatbots}
There are numerous scientific articles and reports on using ChatBots in education, which are mentioned in many studies 
 \cite{Crutzen2011,Kerly2008,Knill2004,Kowalski2013,Jia2009,Gulenko,Imran2014,Kerly2007,Reed2011,Rossi2011}. Even though the names and definitions varies e.g.~ChatBot, \gls{vt}, 
\gls{ia} and \gls{its}, the main purpose is mostly related to either relieving the teacher of work or to aid the user/students to learn more and acquire new knowledge. In the 
papers by \cite{Kerly2008,Knill2004,Kerly2007} they found that students also wanted the ability to do smalltalk and have off-topic conversations. This would be a useful thing 
to implement, since this can increase the chance that the students will use the Chat Agent, since it will not be restricted to just the curriculum (e.g. being able to ask about 
the weather or just random conversations). There can however also be issues with having too free conversations, since users can attempt to use offensive language, invalid input 
causing the application to hang, spelling/grammatical errors or abuse in some way way (\citet{Kerly2008}). Issues can also come if the knowledge base used is outdated, or is based 
on resources where there is no proper control of who is adding the information (\citet{Knill2004,Imran2014,Reed2011}).
\vspace{0.5em}\newline
All information available in the StackExchange community is based on knowledge from the users who posts their answers there. This means that answers can be both outdated and 
invalid. However, StackExchange consists mostly of professional sites, where both moderators and the members are actively following the all posts, be it questions, answers or 
comments. Answers can also be graded by giving votes, and the answer that solved the users problem can be marked as correct. This data can then be used to ensure that the 
solution the Chat Agent presents to the user is based on useful knowledge (e.g. by filtering out answers with votes below a set threshold). An additional filtering can also 
be added by looking at the users reputation and badges \cite{Stackoverflow.com2015d,Stackoverflow.com2015e,CommunityWiki2015a}. Badges are awarded based on your contribution 
to the community, whereas reputation represents how much the community trusts you.
\vspace{0.5em}\newline
The goal is not for the Chat Agent to function as a \gls{vt}, but more of an aiding tool to help students with the more general problems and help them be better at phrasing 
their questions. Not only that, but it can also help the teachers to understand how students learn by looking at the questions they ask the Chat Agent (\citet{Knill2004,Rossi2011}). 
The papers does not list a direct scientific proof that there is a learning improvement by using ChatBots. However, this does not mean that the use of ChatBots cannot have a 
positive impact. As noted in \citet{Kowalski2013}, the quantitative analysis showed no difference between those using and not using a ChatBot, but qualitatively they found that 
the use of a ChatBot was well received, and were open for using it again in the future.

%% partially covers A/B testing, see also next chapter

\section{What is the quality of the results when using \glsentrylong{hmm} (\glsentryshort{hmm}) and \glsentrylong{bn} (\glsentryshort{bn})?}
\label{chapter3:quality_results_hmm_bn}
\emph{TODO: Write this section}
%% In a given amount of executed queries, how many correct results were presented to the user? 

\section{Passing a limited Turing test}
\label{chapter3:turing_test}
The Turing Test (or Imitation Game) is based on the paper by \citet{Turing1998}. In this paper, Turing discusses whether or not a machine can be defined as intelligent, and to 
what ends the intelligence can be measured as. The original Imitation game was based on a man, a woman and a judge, where the goal was for the judge to guess which gender belonged 
to which participant. Turings suggestion was to alter this test to then include a human, a machine and a judge, where the judge would decide whether or not he was talking to a 
human or machine. However, \citet{Harnad2000} argues that the Turing Test is outdated and that a machine easily can trick another human into passing the test. Harnad therefore 
defines five levels\footnote{The levels are t1: "Toy" functionality, T2: Pen-pal function, T3: Sensors and motoric (e.g. robot), T4: Humanoid in both looks and appearance and 
	T5: Grand Unified Theory of Everything (\citet{Harnad2000}).} for the Turing test, where the level starts with t1 ("toy" functionality) and goes up to T5 (Grand Unified Theory 
of Everything). 
Based on Harnads paper, for a ChatBot it would be sufficient to pass T2.
\vspace{0.5em}\newline
Today, the Loebner Contest has replaced the Turing test (\citet{Shieber1994,Zdenek2001}). In the Loebner Contest, the contestants are graded on a numeric scale, where those that 
gets the highest score are perceived as most human-like. The winner was the one with the highest average score. The question however is if these types of tests truly can judge 
intelligence, since most of the time, it is all about the illusion of intelligence (\citet{Livingstone2006, Shieber1994}). Furthermore, often when attempting to reply by using 
the users input as base, it can often produce weird sentences (\citet[p.~6]{Shieber1994}). To account for the lack of intelligence in the beginning, the judges had a script they 
had to stick to, but today the conversations can easily go out of proportions (\citet[p.~13]{Zdenek2001}).
\vspace{0.5em}\newline
In summary, to pass a limited Turing test (e.g. Loebner Contest) it is all about whether or not you can fool the judge. Not about the intelligence. However, intelligence is not 
a key element in the Chat Agent, as most of the "intelligence" will be directed towards giving the students answers that are relevant and meaningful. Considering the goal is 
only to present meaningful answers, the students may be more forgiving when it comes to the probability of being presented with weirdly constructed sentences.

\section{\glsentrylong{qa} (\glsentryshort{qa}): What defines a good question?}
\label{chapter3:define_good_question}
Question can be defined in many ways (e.g. a subject-related, situational, research/thesis, etc) (\citet[p.~3]{Boyer2010}). The main focus is on the academical questions, e.g. 
valid questions when writing the problem statement for the Bachelor or Master thesis. Basically questions of a level to be expected when pursuing an academical degree. 
StackOverflow alone has a lot of pages of how questions should and should not be phrased  \cite{Stackoverflow.com2015,CommunityWiki2015,Stackoverflow.com2015a,Stackoverflow.com2015b,Stackoverflow.com2015c}. 
\citet{Lezina2013} attempted to predict closed questions on StackOverflow by analysing a database dump\footnote{You can also download all data available on StackExchange through 
	the BitTorrent link found here: \url{https://archive.org/details/stackexchange} (last accessed \today).}, but they noted that it would be too time consuming to analyse 
everything. \citet{Slowiaczek1992} researches hypothesis testing, and what defines a good question and answer when you are restricted to only yes and no answers.
\vspace{0.5em}\newline
\citet{Ragonis2013} analysed problem-solving question which were sorted into categories and keywords. This can be used to see if it will be possible to create a taxonomy for 
questions. \citet{Boyer2010} analysed how to encourage problem-solving in students, where they should starting with thinking about whether or not they understand the problem, 
before they attempt to find a solution. They also introduce new ways for instructors to ask questions to help students understand whether or not they understand the current task 
they are given. This can be useful when analysing the questions students asked compared to questions teachers ask. If the student uses an existing question from edX (asked by the 
teacher), presuming that a valid answer is not given, in what way will the student then re-phrase their question in an attempt to find the answer? 
\vspace{0.5em}\newline
By having the Chat Agent as an alternative to the teachers, the students can also improve their own question quality. Students may feel that the question they ask is wrong, too 
stupid, or fear that may be ridiculed when asking it. Through the anonymity of the Chat Agent, they can ask the question in whatever form they want. The answer they get will be 
based on the question they ask, so in time they may improve as they learn what type of question format gives them the answer they seek. 

\begin{comment}
I would argue that there are many different types of question. 
There are structured queries, where can be object, which query to longer to process. 
The next layer is information gathering questions. Questions designed to get information.
\end{comment}
