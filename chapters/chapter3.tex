\chapter{Related work}
\label{chapter3:related_work}

%% 3-10 pages

\section{Comparison of \glsentrylong{ir} (\glsentryshort{ir}) when using Chat Agents and Search engines}
\label{chapter3:chatbot_vs_search_engine}
%% user experience with chatbots vs. search engines
I could not find anything in relation to this topic on the following sites:
\begin{itemize}
	\item \url{http://dblp.uni-trier.de/}
	\item \url{http://link.springer.com/}
	\item \url{http://ieeexplore.ieee.org/}
	\item \url{http://www.sciencedirect.com/}
	\item \url{http://dl.acm.org/}
\end{itemize}
The following is a list of the keyword searches that were made:
\begin{itemize}
	\item 'chatbot vs search engine'
	\item 'chatbot and search engine'
	\item 'comparison of chatbot and search engine'
	\item 'evaluation of retrieval systems' 
\end{itemize}
The reason for this might be related to the fact that search engines are able to retrieve all sorts of information from numerous documents and web-sites, whereas 
ChatBots are usually made for light conversation or for specific topics and purposes. The research question this is related to (Research question \ref{res_q1}) 
is more on the qualitative side, ie. will the users continue to use the search engine, or would they switch to the Chat Agent?
\vspace{0.5em}\newline
Although it is not an evaluation, in \citet{Crutzen2011} a comparison of the ChatBot Bzz (for Windows Live Messenger) is made against search engines and information lines. 
The goal is to see which is better at answering adolescents' questions related to sex, drugs, and alcohol. The comparison was done by giving the users a questionnaire with 
a 5-point Likert scale. The results showed that the users found the ChatBot to be faster and more anonymous, in addition to being easier to use. Information quantity was 
considered less then both information lines and search engines, and it performed better when it came to conciseness and information quality \citet[p.~517-518]{Crutzen2011}.
\vspace{0.5em}\newline
If one compares this paper to the goal of the Chat Agent I plan to develop, one can see some similarities. Even though search engines can give you numerous results, the quality 
of the results may vary, and there may not be any correlation between what you are looking for and what you find. Whereas with my Chat Agent, the focus is only on the StackExchange 
community, specifically on programming and StackOverflow. Therefore, one could also argue that rather then comparing the Chat Agent against a search engine, perhaps it rather should 
be compared against StackExchange. E.g.~comparing the results based on the question asked in the Chat Agent vs.~the question searched for on the given StackExchange site.
\vspace{0.5em}\newline
There is also a program called FAQ FINDER, which is documented in \citet{Burke1997}. As with my Chat Agent, here users can phrase their questions as they would when asking another 
person, rather then use keywords (as they perhaps would had to when using a search engine\footnote{It should also be noted that I am biased towards it being better to phrase questions 
	to find answers, rather then having to enter a list of keywords.}). 

\section{Chat Agents for Learning and Education}
\label{chapter3:learning_with_chatbots}
There are numerous scientific articles and reports on using ChatBots in education, which are mentioned in many studies 
 \cite{Crutzen2011,Kerly2008,Knill2004,Kowalski2013,Jia2009,Gulenko,Imran2014,Kerly2007,Reed2011,Rossi2011}. Even though the names and definitions varies e.g.~ChatBot, \gls{vt}, 
\gls{ia} and \gls{its}, the main purpose is mostly related to either relieving the teacher of work or to aid the user/students to learn more and acquire new knowledge. In the 
papers by \cite{Kerly2008,Knill2004,Kerly2007} they found that students also wanted the ability to do smalltalk and have off-topic conversations. This would be a useful thing 
to implement, since this can increase the chance that the students will use the Chat Agent, since it will not be restricted to just the curriculum (e.g. being able to ask about 
the weather or just random conversations). There can however also be issues with having too free conversations, since users can attempt to use offensive language, invalid input 
causing the application to hang, spelling/grammatical errors or abuse in some way way (\citet{Kerly2008}). Issues can also come if the knowledge base used is outdated, or is based 
on resources where there is no proper control of who is adding the information (\citet{Knill2004,Imran2014,Reed2011}).
\vspace{0.5em}\newline
All information available in the StackExchange community is based on knowledge from the users who posts their answers there. This means that answers can be both outdated and 
invalid. However, StackExchange consists mostly of professional sites, where both moderators and the members are actively following the all posts, be it questions, answers or 
comments. Answers can also be graded by giving votes, and the answer that solved the users problem can be marked as correct. This data can then be used to ensure that the 
solution the Chat Agent presents to the user is based on useful knowledge (e.g. by filtering out answers with votes below a set threshold). An additional filtering can also 
be added by looking at the users reputation and badges \cite{Stackoverflow.com2015d,Stackoverflow.com2015e,CommunityWiki2015a}. Badges are awarded based on your contribution 
to the community, whereas reputation represents how much the community trusts you.
\vspace{0.5em}\newline
The goal is not for the Chat Agent to function as a \gls{vt}, but more of an aiding tool to help students with the more general problems and help them be better at phrasing 
their questions. Not only that, but it can also help the teachers to understand how students learn by looking at the questions they ask the Chat Agent (\citet{Knill2004,Rossi2011}). 
The papers does not list a direct scientific proof that there is a learning improvement by using ChatBots. However, this does not mean that the use of ChatBots cannot have a 
positive impact. As noted in \citet{Kowalski2013}, the quantitative analysis showed no difference between those using and not using a ChatBot, but qualitatively they found that 
the use of a ChatBot was well received, and were open for using it again in the future.

%% partially covers A/B testing, see also next chapter

\section{What is the quality of the results when using \glsentrylong{hmm} (\glsentryshort{hmm}) and \glsentrylong{bn} (\glsentryshort{bn})?}
\label{chapter3:quality_results_hmm_bn}
The following is a short introduction to \gls{hmm} and \gls{bn}. Both \gls{hmm} and \gls{bn} is a statistical model. \gls{bn} is a generalization of the Bayesian 
classifier\footnote{Bayesian classifier calculates conditional probabilities based on the attribute values. It assumes attributes have conditional independence, and 
	that they are discrete. If they are continuous, they have to be discretized (\citet[p.~7]{Kononenko2007}).}, which uses acyclic graphs to model dependencies. 
 (\citet[p.~249-256]{Kononenko2007}, \citet{Ghahramani2001}). The nodes\footnote{However, if a given node is at the beginning (no nodes that link to the it), 
	it is called parent attribute.} in \gls{bn} represents the attribute value, and the edge is the correlation between them. 
\citet[p.~180]{Mitchell1997} presents a Naive Bayes algorithm for classifying English text on the web with an accuracy of 89\%.
%% TODO: Write more about tests/accuracy on Bayes Net?
\vspace{0.5em}\newline
\gls{hmm}\footnote{If \gls{hmm} only consists of a single state and variable, it is an Dynamic \gls{bn}. Dynamic \gls{bn} is useful if one can reduce the state 
	to a single variable, or if the states only rely on a single parent. This can then be used to reduce the temporal probability model (\citet[p.~600-601]{Russell2013}). } 
is based on the Markov model\footnote{The Markov model consists of states that change over time based on probability. The current state decides the probability for the next 
	one (\cite[p.~3]{Moore}).}, but \gls{hmm} also contains hidden states (\citet{Ghahramani2001, Russell2013}). An example would be the effect weather has on the environment, 
where the observable states would be whether the ground is wet, dry or frozen, and the hidden states would be sunny, cloudy, rainy and snowy (if the temperature also was a 
factor to consider, it would be observable). If the weather is sunny or cloudy, the ground will most likely be dry. If the weather is rainy or snowy, the ground could then 
be both wet and/or frozen (e.g. it rains, and the temperature drops). This could then be used to create a probabilistic model to see what state the ground would be in based 
on the current weather condition.
\vspace{0.5em}\newline
\citet{Gao1999} analysed system errors in the states of \gls{hmm} (for Speech-Recognition), where they found that states could be misclassified (non-aggressive condition) 
or wrongly enter the space of other states (invasive condition). They found that if there was a large amount of non-aggressive states, a slightly larger Gaussian mixture was 
better. 
\vspace{0.5em}\newline
\citet{Pan2015} developed a \gls{hmm} thesaurus for extracting terms from academic Chinese literature. \citet{Yongjin2009} developed a symbol feature-based \gls{hmm} for English 
pages using Viterbi algorithm\footnote{Viterbi algorithm can be thought of as Occam's Razor. If transition of states presents the same pattern, then this should be true for all 
	who match that pattern.}. \citet{Zhou2010} researched the use of \gls{hmm} for information extraction from HTML documents. They found that if you cleaned the HTML data and 
extracted the data by using the Viterbi algorithm, you would get better extraction results.
\vspace{0.5em}\newline
\citet{Barros2008} presents a hybrid \gls{hmm} for information extraction from 6000 references. They tested the \gls{hmm} against four different 
classifiers\footnote{They used Weka (\url{http://sourceforge.net/projects/weka/}), and the four classifiers were: PART, Naive Bayes, k-Nearest Neighbour (k-NN) and Support Vector 
	Machine (SVM).}, where they found the average precision of \gls{hmm} and Naive Bayes to be 72.38\% (but k-NN scored highest with 76.15\%; \cite[p.~5]{Barros2008}).
 \citet{Seymore1999} used \gls{hmm} and the Viterbi algorithm (for recovery of state 
sequence) to retrieve header information from computer science research papers. They achieved an overall accuracy of 92.9\% for the all classes in the header\footnote{The 
	class-specific accuracies were 97.8\% for titles and 97.2\% for authors (\cite[p.~6]{Seymore1999}).} 
\newpage\noindent
These papers shows that \gls{hmm} is fitting for the purpose of \gls{ir} and information extraction, and that there are different ways this can be achieved. 
My goal is to use both \gls{hmm} and \gls{bn}, and it will most likely benefit the most from using the Viterbi algorithm (e.g. recover state sequence and improve 
conversations). Additional extensions could also be added (e.g. Reinforcement learning for improved \gls{ir}) to improve the results given to the user.

\section{Passing a limited Turing test}
\label{chapter3:turing_test}
The Turing Test (or Imitation Game) is based on the paper by \citet{Turing1998}. In this paper, Turing discusses whether or not a machine can be defined as intelligent, and to 
what ends the intelligence can be measured as. The original Imitation game was based on a man, a woman and a judge, where the goal was for the judge to guess which gender belonged 
to which participant. Turings suggestion was to alter this test to instead include a human, a machine, and a judge, where the judge would decide whether or not he was talking to a 
human or machine. However, \citet{Harnad2000} argues that the Turing Test is outdated and that a machine easily can trick another human into passing the test. Harnad therefore 
defines five levels\footnote{The levels are t1: "Toy" functionality, T2: Pen-pal function (e.g. ChatBot), T3: Sensors and motoric (e.g. robot), T4: Humanoid (in both looks and 
	appearance) and T5: Grand Unified Theory of Everything (\citet{Harnad2000}).} for the Turing test, where the level starts with t1 ("toy" functionality) and goes up to T5 
(Grand Unified Theory of Everything). Based on Harnads paper, for my Chat Agent it would be sufficient to pass T2.
\vspace{0.5em}\newline
Today, the Loebner Contest has replaced the Turing test (\citet{Shieber1994,Zdenek2001}). In the Loebner Contest, the contestants are graded on a numeric scale, where those that 
gets the highest score are perceived as most human-like. The winner was the one with the highest average score. The question however is if these types of tests truly can judge 
intelligence, since most of the time, it is all about the illusion of intelligence (\citet{Livingstone2006, Shieber1994}). The ChatBots could also end up creating weird 
sentences when basing their output on the users input (e.g. \citet[p.~6]{Shieber1994}). In the early days, the judges had to stick to a given script, but today the conversations 
can easily go out of proportions (\citet[p.~13]{Zdenek2001}).
\vspace{0.5em}\newline
Trying to pass a limited Turing test (e.g. Loebner Contest) seems to be more about fooling the judges by providing meaningful answers, rather then displaying intelligence. It is 
debatable whether or not my Chat Agent needs to pass such a test, considering the students already know that they are talking to a Chat Agent and not a real person\footnote{An 
	interesting experiment (as a continuation after the thesis, for the next years students) would be to tell the experimental group that they are 
	testing out a new \gls{qa} system (where they can get help from a domain expert). This group could then be expanded into two, where the first half is talking to a real 
	person and the other half is talking to the Chat Agent. This could then be used to compare the accuracy of the answers provided by the Chat Agent vs. the person, in addition 
	to the students at the end being asked to judge whether they talked to a real person (e.g. a teacher) or the Chat Agent. To avoid bias, this should be executed as a double 
	blind.}.
Although the conversations should be as humanoid as possible, the most important part is that the answers are meaningful and relevant to the students questions.
\vspace{0.5em}\newline
One could of course separate the conversation pattern into two parts; one for the \gls{qa} and the other for small-talk/off-topic conversations. However, the small-talk would  
not be a priority, so if this were to be added in the first release (mid-February), it would need to be initiated through a command. This command could then later be replaced by 
using semantics to separate between \gls{qa} and small-talk. If small-talk was a feature the students wanted to see more of, improvements could then be made by asking them for 
suggestions and feedback. E.g.~What do you think of the conversations? In what ways can the conversations be improved? etc.

\section{\glsentrylong{qa} (\glsentryshort{qa}): What defines a good question?}
\label{chapter3:define_good_question}
Question can be defined in many ways (e.g. a subject-related, situational, research/thesis, etc) (\citet[p.~3]{Boyer2010}). The focus of my Master thesis are questions on an 
academic level (Bachelor and Master). StackOverflow has several pages with guidelines for asking (good) questions 
 \cite{Stackoverflow.com2015,CommunityWiki2015,Stackoverflow.com2015a,Stackoverflow.com2015b,Stackoverflow.com2015c}. 
 \vspace{0.5em}\newline
\citet{Lezina2013} attempted to predict closed questions on StackOverflow by analysing a database dump\footnote{You can also download all data available on StackExchange through 
	the BitTorrent link found here: \url{https://archive.org/details/stackexchange} (last accessed \today).}, but they noted that it would be too time consuming to analyse 
everything. \citet{Slowiaczek1992} researches hypothesis testing, and what defines a good question and answer when you are restricted to only yes and no answers. 
\citet{Movshovitz-Attias2013} analysed the reputations of users and their contribution by creating a User Activity Model (UAM), where they found a clear difference between 
expert and non-expert users.
\vspace{0.5em}\newline
\citet{Ragonis2013} analysed problem-solving question which were sorted into categories and keywords. This can be used to see if it will be possible to create a taxonomy for 
questions (as was done in \citet{Nielsen2008}). \citet{Boyer2010} analysed how to encourage problem-solving in students. One of their findings was that students should start with 
thinking about whether or not they understand the problem, before attempting to find a solution. \citet{Boyer2010} also suggests that instructors should start by asking 
questions to see if the students truly understand the task they are given. 

This can be useful when analysing the students questions, since I can compare their questions against those from the edX course. E.g.~if the student starts with asking a question 
taken from an edX course (assuming an answer is not found), in what way will the student phrase their question to find the answer? 
\vspace{0.5em}\newline
By having the Chat Agent as an alternative to the teachers, the students can also improve their own question quality. Students may feel that the question they ask is wrong, too 
stupid, or fear that may be ridiculed when asking it. Through the anonymity of the Chat Agent, they can ask the question in whatever form they want. The answer they get will be 
based on the question they ask, so in time they may improve as they learn what type of question format gives them the answer they seek. 

\begin{comment}
I would argue that there are many different types of question. 
There are structured queries, where can be object, which query to longer to process. 
The next layer is information gathering questions. Questions designed to get information.
\end{comment}
